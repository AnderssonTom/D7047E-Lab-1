{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnderssonTom/D7047E-Lab-1/blob/main/D7047E_Lab_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#D7047E Lab 1\n",
        "##Group 25\n",
        "Antonino Davolos, Christos Michail, Felix Hessinger, Sandra Sandström, Tom Andersson"
      ],
      "metadata": {
        "id": "o10I4A60vX5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##GPU and memory info on Colab"
      ],
      "metadata": {
        "id": "MNjf5h3qv7-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrrhfo0fozJy",
        "outputId": "51262d8c-8772-4216-d5a3-2fd9616608fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Apr  5 07:45:11 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8             30W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "Your runtime has 13.6 gigabytes of available RAM\n",
            "\n",
            "Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1.1. Simple model, without/with embeddings\n",
        "###ANN: 5000 X 64 X 2\n",
        "###Without embeddings (TF-IDF), single words and word-pairs as features, encoded in terms of their importance (sentence distinctive frequencies)"
      ],
      "metadata": {
        "id": "bK9cYGMhwIgN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3UiA-7i_nOC",
        "outputId": "ad95faae-c37e-4782-a347-5604f13b7301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 20.0334\n",
            "Epoch 2, Loss: 18.6824\n",
            "Epoch 3, Loss: 15.2012\n",
            "Epoch 4, Loss: 10.7022\n",
            "Epoch 5, Loss: 6.5438\n",
            "\n",
            "Validation Accuracy: 0.7600\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.78      0.76        50\n",
            "           1       0.77      0.74      0.76        50\n",
            "\n",
            "    accuracy                           0.76       100\n",
            "   macro avg       0.76      0.76      0.76       100\n",
            "weighted avg       0.76      0.76      0.76       100\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def preprocess_pandas(data, columns):\n",
        "    df_ = pd.DataFrame(columns=columns)\n",
        "    data['Sentence'] = data['Sentence'].str.lower()\n",
        "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
        "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
        "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','', regex=True)                                                       # remove special characters\n",
        "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
        "    for index, row in data.iterrows():\n",
        "        word_tokens = word_tokenize(row['Sentence'])\n",
        "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        df_.loc[len(df_)] = {\n",
        "            \"index\": row['index'],\n",
        "            \"Class\": row['Class'],\n",
        "            \"Sentence\": \" \".join(filtered_sent)\n",
        "        }\n",
        "    return df_\n",
        "\n",
        "# === Load and Preprocess Data ===\n",
        "data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
        "data.columns = ['Sentence', 'Class']\n",
        "data['index'] = data.index\n",
        "columns = ['index', 'Class', 'Sentence']\n",
        "data = preprocess_pandas(data, columns)\n",
        "\n",
        "# === Split Data ===\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
        "    data['Sentence'].values.astype('U'),\n",
        "    data['Class'].values.astype('int32'),\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# === TF-IDF Vectorization ===\n",
        "vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), max_features=5000)\n",
        "train_features = vectorizer.fit_transform(train_sentences).todense()\n",
        "val_features = vectorizer.transform(val_sentences).todense()\n",
        "\n",
        "# === Convert to Tensors ===\n",
        "train_x = torch.tensor(np.array(train_features)).float()\n",
        "train_y = torch.tensor(np.array(train_labels)).long()\n",
        "val_x = torch.tensor(np.array(val_features)).float()\n",
        "val_y = torch.tensor(np.array(val_labels)).long()\n",
        "\n",
        "train_dataset = TensorDataset(train_x, train_y)\n",
        "val_dataset = TensorDataset(val_x, val_y)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# === Define a Simple ANN ===\n",
        "class SimpleANN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(SimpleANN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 2)  # Two output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.fc1(x))\n",
        "        return self.fc2(out)\n",
        "\n",
        "# === Initialize and Train ===\n",
        "model = SimpleANN(input_dim=train_x.shape[1])\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# === Training Loop ===\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# === Evaluate ===\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_preds = []\n",
        "    val_true = []\n",
        "    for inputs, targets in val_loader:\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        val_preds.extend(preds.tolist())\n",
        "        val_true.extend(targets.tolist())\n",
        "\n",
        "print(f\"\\nValidation Accuracy: {accuracy_score(val_true, val_preds):.4f}\")\n",
        "print(classification_report(val_true, val_preds, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model with word embeddings and sentence embeddings based on average pooling"
      ],
      "metadata": {
        "id": "c0gVe0VF0F6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Download required resources ===\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# === Load Data ===\n",
        "df = pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter=\"\\t\", header=None, names=[\"Sentence\", \"Class\"])\n",
        "\n",
        "# === Clean and Tokenize ===\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', '', text)  # remove emails\n",
        "    text = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', text)  # remove IP addresses\n",
        "    text = re.sub(r'\\d+', '', text)  # remove numbers\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['Sentence'].apply(clean_text)\n",
        "\n",
        "# === Build Vocabulary ===\n",
        "all_tokens = [token for sentence in df['tokens'] for token in sentence]\n",
        "vocab_counts = Counter(all_tokens)\n",
        "vocab = {word: idx + 2 for idx, (word, _) in enumerate(vocab_counts.items())}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# === Encode Sentences ===\n",
        "def encode_sentence(tokens, vocab, max_len=50):\n",
        "    encoded = [vocab.get(word, vocab['<UNK>']) for word in tokens]\n",
        "    if len(encoded) < max_len:\n",
        "        encoded += [vocab['<PAD>']] * (max_len - len(encoded))\n",
        "    else:\n",
        "        encoded = encoded[:max_len]\n",
        "    return encoded\n",
        "\n",
        "df['encoded'] = df['tokens'].apply(lambda x: encode_sentence(x, vocab))\n",
        "\n",
        "# === Split Data ===\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['encoded'].tolist(), df['Class'].tolist(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# === Create Dataset ===\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = ReviewDataset(X_train, y_train)\n",
        "val_dataset = ReviewDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "\n",
        "# === Define Model with Trainable Embeddings ===\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=64):\n",
        "        super(TextClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
        "        pooled = embedded.mean(dim=1)  # Average over sequence\n",
        "        x = self.relu(self.fc1(pooled))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# === Initialize Model ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TextClassifier(vocab_size=len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# === Train Model ===\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in tqdm(train_loader):\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# === Evaluate ===\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in val_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "print(f\"\\nValidation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
        "print(classification_report(all_labels, all_preds, zero_division=0))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhOrYQGSYn5n",
        "outputId": "a27ff9b6-7587-4060-fd06-9d4b4a5473b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100%|██████████| 352/352 [00:01<00:00, 226.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 187.2570\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:00<00:00, 368.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5, Loss: 127.0114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:00<00:00, 378.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5, Loss: 103.5700\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:00<00:00, 375.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/5, Loss: 86.9367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 352/352 [00:00<00:00, 375.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/5, Loss: 73.6573\n",
            "\n",
            "Validation Accuracy: 0.8520\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.83      0.81       965\n",
            "           1       0.89      0.86      0.88      1535\n",
            "\n",
            "    accuracy                           0.85      2500\n",
            "   macro avg       0.84      0.85      0.85      2500\n",
            "weighted avg       0.85      0.85      0.85      2500\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model with LSTM classifier\n",
        "The LSTM model takes word order into account. However, it needed several adjustments to reach a reasonable accuracy. Further testing showed that we only needed to reverse the input array to reach high accuracy. See code following this one. This indicates that it is actually better to skip order sensitivity. Average pooling is simpler and even more accurate."
      ],
      "metadata": {
        "id": "yRhNi9wn1dqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Setup ===\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# === Load and Preprocess Data ===\n",
        "df = pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter=\"\\t\", header=None, names=[\"Sentence\", \"Class\"])\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', '', text)  # remove emails\n",
        "    text = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', text)  # remove IPs\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['Sentence'].apply(clean_text)\n",
        "\n",
        "# === Build Vocabulary ===\n",
        "all_tokens = [token for sentence in df['tokens'] for token in sentence]\n",
        "vocab_counts = Counter(all_tokens)\n",
        "vocab = {word: idx + 2 for idx, (word, _) in enumerate(vocab_counts.items())}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# === Encode Tokens ===\n",
        "MAX_LEN = 40  # reduce padding length\n",
        "def encode_sentence(tokens, vocab, max_len=MAX_LEN):\n",
        "    encoded = [vocab.get(word, vocab['<UNK>']) for word in tokens]\n",
        "    return encoded[:max_len] + [vocab['<PAD>']] * (max_len - len(encoded))\n",
        "\n",
        "df['encoded'] = df['tokens'].apply(lambda x: encode_sentence(x, vocab))\n",
        "\n",
        "# === Split Dataset ===\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['encoded'].tolist(), df['Class'].tolist(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# === Dataset Class ===\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = ReviewDataset(X_train, y_train)\n",
        "val_dataset = ReviewDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # reduced batch size\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# === LSTM Model ===\n",
        "class OptimizedLSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=128, dropout_p=0.3):\n",
        "        super(OptimizedLSTMClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, 2)  # *2 for bidirectional\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)                     # (batch, seq_len, embed_dim)\n",
        "        _, (hidden, _) = self.lstm(embedded)             # hidden: (2, batch, hidden_dim)\n",
        "        combined = torch.cat((hidden[0], hidden[1]), dim=1)  # concat both directions\n",
        "        output = self.dropout(combined)\n",
        "        return self.fc(output)\n",
        "\n",
        "# === Training Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = OptimizedLSTMClassifier(vocab_size=len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# === Training Loop ===\n",
        "EPOCHS = 10\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1} Loss: {total_loss:.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in val_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "print(f\"\\nValidation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
        "print(classification_report(all_labels, all_preds, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMIJ8_t9f2no",
        "outputId": "d086ca1c-1adf-4bf7-ff57-aaacf2d22c62"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Epoch 1: 100%|██████████| 704/704 [00:03<00:00, 216.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 331.6123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 704/704 [00:02<00:00, 259.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 Loss: 226.2434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 704/704 [00:02<00:00, 276.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 Loss: 163.7780\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 704/704 [00:03<00:00, 233.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Loss: 105.9850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 704/704 [00:02<00:00, 276.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Loss: 56.2301\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 704/704 [00:02<00:00, 276.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Loss: 29.1531\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 704/704 [00:02<00:00, 272.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Loss: 20.5698\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 704/704 [00:04<00:00, 146.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Loss: 10.3566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 704/704 [00:02<00:00, 274.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Loss: 14.3603\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 704/704 [00:02<00:00, 273.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Loss: 9.2972\n",
            "\n",
            "Validation Accuracy: 0.8364\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.77      0.78       965\n",
            "           1       0.86      0.88      0.87      1535\n",
            "\n",
            "    accuracy                           0.84      2500\n",
            "   macro avg       0.83      0.82      0.83      2500\n",
            "weighted avg       0.84      0.84      0.84      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###LSTM with just reversed order of input."
      ],
      "metadata": {
        "id": "JV5_oKAZ133f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Setup ===\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# === Load Data ===\n",
        "df = pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter=\"\\t\", header=None, names=[\"Sentence\", \"Class\"])\n",
        "\n",
        "# === Text Cleaning and Tokenization ===\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return tokens\n",
        "\n",
        "df['tokens'] = df['Sentence'].apply(clean_text)\n",
        "\n",
        "# === Vocabulary Building ===\n",
        "all_tokens = [token for sentence in df['tokens'] for token in sentence]\n",
        "vocab_counts = Counter(all_tokens)\n",
        "vocab = {word: idx + 2 for idx, (word, _) in enumerate(vocab_counts.items())}\n",
        "vocab['<PAD>'] = 0\n",
        "vocab['<UNK>'] = 1\n",
        "\n",
        "# === Encode Tokens ===\n",
        "def encode_sentence(tokens, vocab, max_len=50):\n",
        "    encoded = [vocab.get(word, vocab['<UNK>']) for word in tokens]\n",
        "    if len(encoded) < max_len:\n",
        "        encoded += [vocab['<PAD>']] * (max_len - len(encoded))\n",
        "    else:\n",
        "        encoded = encoded[:max_len]\n",
        "    return encoded\n",
        "\n",
        "df['encoded'] = df['tokens'].apply(lambda x: encode_sentence(x, vocab))\n",
        "\n",
        "# === Split Dataset ===\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    df['encoded'].tolist(), df['Class'].tolist(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# === Dataset Class ===\n",
        "class ReviewDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = ReviewDataset(X_train, y_train)\n",
        "val_dataset = ReviewDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# === LSTM-based Classifier ===\n",
        "class TextClassifierLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=100, hidden_dim=64):\n",
        "        super(TextClassifierLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)                # (batch, seq_len, embedding_dim)\n",
        "        _, (hidden, _) = self.lstm(embedded)        # hidden: (1, batch, hidden_dim)\n",
        "        pooled = hidden[-1]                         # take last hidden state\n",
        "        return self.fc(pooled)\n",
        "\n",
        "# === Training Setup ===\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = TextClassifierLSTM(vocab_size=len(vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# === Training Loop ===\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in tqdm(train_loader):\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# === Evaluation ===\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in val_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        outputs = model(batch_X)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(batch_y.cpu().numpy())\n",
        "\n",
        "print(f\"\\nValidation Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
        "print(classification_report(all_labels, all_preds, zero_division=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CYaSXTkAyRo",
        "outputId": "88e3ef44-2655-4d6a-ffb8-b058b53704f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "100%|██████████| 704/704 [00:02<00:00, 288.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 335.2682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:02<00:00, 240.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 222.3020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:02<00:00, 294.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 156.5206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:02<00:00, 290.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 101.0477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:02<00:00, 291.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 56.9731\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:02<00:00, 279.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 29.8966\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:03<00:00, 229.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 18.8335\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:02<00:00, 265.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 9.7206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:02<00:00, 292.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 10.1919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 704/704 [00:02<00:00, 291.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 6.6553\n",
            "\n",
            "Validation Accuracy: 0.8340\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.76      0.78       965\n",
            "           1       0.85      0.88      0.87      1535\n",
            "\n",
            "    accuracy                           0.83      2500\n",
            "   macro avg       0.83      0.82      0.82      2500\n",
            "weighted avg       0.83      0.83      0.83      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1.2. Transformer model, without/with BERT pretraining\n",
        "###No pretraining\n"
      ],
      "metadata": {
        "id": "FvJgVNml2Gpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== 1. Imports ==================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# ================== 2. Load and Preprocess Data ==================\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', '', text)  # remove emails\n",
        "    text = re.sub(r'((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', text)  # remove IP addresses\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # remove punctuation\n",
        "    text = re.sub(r'\\d+', '', text)  # remove digits\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    filtered = [w for w in tokens if w not in stopwords.words('english')]\n",
        "    return filtered\n",
        "\n",
        "# Load and process\n",
        "df = pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter=\"\\t\", header=None)\n",
        "df.columns = [\"text\", \"label\"]\n",
        "df.dropna(inplace=True)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df[\"tokens\"] = df[\"text\"].apply(clean_and_tokenize)\n",
        "\n",
        "# ================== 3. Build Vocabulary and Encode ==================\n",
        "\n",
        "all_tokens = [token for sent in df[\"tokens\"] for token in sent]\n",
        "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "vocab.update({word: idx + 2 for idx, (word, _) in enumerate(Counter(all_tokens).items())})\n",
        "\n",
        "def encode(tokens, vocab, max_len=32):\n",
        "    ids = [vocab.get(tok, vocab[\"<UNK>\"]) for tok in tokens]\n",
        "    return ids[:max_len] + [vocab[\"<PAD>\"]] * max(0, max_len - len(ids))\n",
        "\n",
        "df[\"input_ids\"] = df[\"tokens\"].apply(lambda x: encode(x, vocab))\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"input_ids\"].tolist(), df[\"label\"].tolist(), test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "# ================== 4. Dataset Class ==================\n",
        "\n",
        "class AmazonDataset(Dataset):\n",
        "    def __init__(self, inputs, labels):\n",
        "        self.inputs = inputs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(self.inputs[idx]),\n",
        "            \"labels\": torch.tensor(self.labels[idx])\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = AmazonDataset(train_texts, train_labels)\n",
        "val_dataset = AmazonDataset(val_texts, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "\n",
        "# ================== 5. Transformer Model ==================\n",
        "\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=64, num_heads=2, hidden_dim=128, num_layers=2, max_len=32):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.embedding(input_ids) + self.pos_embedding[:, :input_ids.size(1), :]\n",
        "        x = self.encoder(x)\n",
        "        pooled = x.mean(dim=1)  # Average pooling\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "# ================== 6. Train ==================\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MiniTransformer(vocab_size=len(vocab)).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(5):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# ================== 7. Evaluate ==================\n",
        "\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "        outputs = model(input_ids)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7mRg1YEtphX",
        "outputId": "11b127bc-193e-4086-e98f-d9842bf3a03a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 405.7679\n",
            "Epoch 2, Loss: 288.4212\n",
            "Epoch 3, Loss: 232.2841\n",
            "Epoch 4, Loss: 196.9329\n",
            "Epoch 5, Loss: 168.7134\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7643    0.8221    0.7921      1006\n",
            "           1     0.8738    0.8293    0.8510      1494\n",
            "\n",
            "    accuracy                         0.8264      2500\n",
            "   macro avg     0.8190    0.8257    0.8216      2500\n",
            "weighted avg     0.8297    0.8264    0.8273      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###With BERT pretraining model"
      ],
      "metadata": {
        "id": "1spZStDm2g0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ================== 1. Install + Imports ==================\n",
        "!pip install -q transformers\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ================== 2. Load and Prepare Data ==================\n",
        "df = pd.read_csv(\"amazon_cells_labelled_LARGE_25K.txt\", delimiter=\"\\t\", header=None)\n",
        "df.columns = [\"text\", \"label\"]\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Optional shuffle\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"text\"].tolist(),\n",
        "    df[\"label\"].tolist(),\n",
        "    test_size=0.1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ================== 3. Tokenization ==================\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=64)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=64)\n",
        "\n",
        "# ================== 4. Dataset Class ==================\n",
        "class AmazonDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            key: torch.tensor(val[idx])\n",
        "            for key, val in self.encodings.items()\n",
        "        } | {\"labels\": torch.tensor(self.labels[idx])}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = AmazonDataset(train_encodings, train_labels)\n",
        "val_dataset = AmazonDataset(val_encodings, val_labels)\n",
        "\n",
        "# ================== 5. Load Model ==================\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "\n",
        "# ================== 6. Training Arguments ==================\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# ================== 7. Metrics Function ==================\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = np.argmax(pred.predictions, axis=1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "# ================== 8. Trainer ==================\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# ================== 9. Train ==================\n",
        "trainer.train()\n",
        "\n",
        "# ================== 10. Evaluate ==================\n",
        "metrics = trainer.evaluate()\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "for k, v in metrics.items():\n",
        "    if k.startswith(\"eval_\"):\n",
        "        print(f\"{k[5:].capitalize()}: {v:.4f}\")\n",
        "\n",
        "# ================== 11. Optional: Classification Report ==================\n",
        "preds_output = trainer.predict(val_dataset)\n",
        "preds = np.argmax(preds_output.predictions, axis=1)\n",
        "labels = preds_output.label_ids\n",
        "\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(labels, preds, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "9LPbXR0JerAg",
        "outputId": "76d2ea7a-2384-4269-abd3-541090d016fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5628' max='5628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5628/5628 20:40, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.237800</td>\n",
              "      <td>0.204519</td>\n",
              "      <td>0.930400</td>\n",
              "      <td>0.955172</td>\n",
              "      <td>0.927041</td>\n",
              "      <td>0.940897</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.160800</td>\n",
              "      <td>0.197779</td>\n",
              "      <td>0.941200</td>\n",
              "      <td>0.945731</td>\n",
              "      <td>0.956493</td>\n",
              "      <td>0.951082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.088100</td>\n",
              "      <td>0.287009</td>\n",
              "      <td>0.935200</td>\n",
              "      <td>0.939314</td>\n",
              "      <td>0.953146</td>\n",
              "      <td>0.946179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.047700</td>\n",
              "      <td>0.321117</td>\n",
              "      <td>0.938800</td>\n",
              "      <td>0.944334</td>\n",
              "      <td>0.953815</td>\n",
              "      <td>0.949051</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluation Metrics:\n",
            "Loss: 0.1978\n",
            "Accuracy: 0.9412\n",
            "Precision: 0.9457\n",
            "Recall: 0.9565\n",
            "F1: 0.9511\n",
            "Runtime: 7.8659\n",
            "Samples_per_second: 317.8290\n",
            "Steps_per_second: 5.0850\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9343    0.9185    0.9263      1006\n",
            "           1     0.9457    0.9565    0.9511      1494\n",
            "\n",
            "    accuracy                         0.9412      2500\n",
            "   macro avg     0.9400    0.9375    0.9387      2500\n",
            "weighted avg     0.9411    0.9412    0.9411      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Task 1.3 Comparison\n",
        "Here, you should compare of both models; you are requested to use the same test dataset for both ANN and the transformer to answer the following:\n",
        "\n",
        "- Compare the performance of the two models and explain in which scenarios you would prefer one over the other.\n",
        "\n",
        "- How did the two models’ complexity, accuracy, and efficiency differ? Did one model outperform the other in specific scenarios or tasks? If so, why?\n",
        "\n",
        "- What insights did you obtain concerning data amount to train?Embedding utilized? Architectural choices made?\n"
      ],
      "metadata": {
        "id": "xrJGOrCd24AO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO30XNVyBRrwvTbuD3plZud",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}